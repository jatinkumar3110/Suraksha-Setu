# -*- coding: utf-8 -*-
"""Model Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MmjPss5VntpYs4RHMFW1SvRievAj7ijb
"""

import numpy as np
import tensorflow as tf
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam

def train_random_forest(X_train, y_train):
    """Trains and returns a Random Forest Classifier."""
    print("Training Random Forest model...")
    model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    print("Random Forest training complete.")
    return model

def train_lstm(X_train, y_train):
    """Builds and trains an LSTM model."""
    print("Building and training LSTM model...")
    X_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))

    model = tf.keras.Sequential([
        Input(shape=(1, X_train.shape[1])),
        LSTM(64, return_sequences=True, activation='relu'),
        LSTM(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train_lstm, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=0,
              callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])
    print("LSTM training complete.")
    return model

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    """Creates a single transformer block."""
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(x + inputs)
    ff_output = Dense(ff_dim, activation="relu")(x)
    ff_output = Dense(inputs.shape[-1])(ff_output)
    ff_output = Dropout(dropout)(ff_output)
    return LayerNormalization(epsilon=1e-6)(x + ff_output)

def train_transformer(X_train, y_train):
    """Builds and trains a Transformer model."""
    print("Building and training Transformer model...")
    X_train_trans = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))

    inputs = Input(shape=(1, X_train.shape[1]))
    x = inputs
    for _ in range(2):
        x = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=128, dropout=0.1)

    x = GlobalAveragePooling1D(data_format="channels_last")(x)
    x = Dense(64, activation="relu")(x)
    outputs = Dense(1, activation="sigmoid")(x)

    model = Model(inputs, outputs)
    model.compile(optimizer=Adam(learning_rate=0.001), loss="binary_crossentropy", metrics=["accuracy"])
    model.fit(X_train_trans, y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0,
              callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])
    print("Transformer training complete.")
    return model

