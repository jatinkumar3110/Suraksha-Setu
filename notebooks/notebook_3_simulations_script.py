# -*- coding: utf-8 -*-
"""Notebook 3 - Simulations Script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mB1YjYWq-AWX9CO_Oi2H6m_G93c614vK
"""

# # Notebook 3: Advanced Simulations and Deployment
#
# **Objective:** To simulate the real-world application of the trained models. This includes generating multilingual alerts for a detected event and simulating the rescue coordination and routing system. We will also prototype a simple API.

# ### Step 1: Import Libraries and Load Models
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import networkx as nx
import random
import sys
import os

# Add src directory to path
sys.path.append(os.path.abspath(os.path.join('..', 'src')))

import utils
import evaluate as ev

# Load the models we saved in the previous notebook
rf_model = utils.load_model('../models/random_forest_v1.pkl')
lstm_model = utils.load_model('../models/lstm_model_v1.h5')
transformer_model = utils.load_model('../models/transformer_model_v1.h5')

models = {
    'rf': rf_model,
    'lstm': lstm_model,
    'transformer': transformer_model
}

# Load the scaler and a sample of test data
with open('../data/processed/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)
X_test = np.load('../data/processed/X_test.npy')
y_test = np.load('../data/processed/y_test.npy')

# Step 2: Simulate a High-Risk Event Prediction
# Let's find a sample from our test set that is a true positive (a real disaster event) and use it for our simulations.
# Find the index of the first true disaster event in the test set
high_risk_index = np.where(y_test == 1)[0][0]

# Get the corresponding data point
sample_data_point_scaled = X_test[high_risk_index].reshape(1, -1)

# Get the prediction from our Hybrid Ensemble model
predictions = ev.get_predictions(models, sample_data_point_scaled)
final_prediction = predictions['Hybrid Ensemble'][0]

print(f"Sample Data Point (Scaled):\n{sample_data_point_scaled}")
print(f"\nEnsemble Prediction: {'Disaster Risk' if final_prediction == 1 else 'No Risk'}")

# ### Step 3: Multilingual NLP Alert Simulation
def simulate_multilingual_alert(prediction, data_point_scaled, scaler):
    """Generates and 'translates' a disaster alert based on a prediction."""
    if prediction == 0:
        print("No alert needed.")
        return

    print("--- Generating Multilingual Disaster Alert ---")

    # Inverse transform to get original values for context
    data_point_original = scaler.inverse_transform(data_point_scaled)[0]
    features = ['rainfall_mm', 'river_level_m', 'seismic_intensity', 'citizen_reports', 'temperature_c', 'humidity_percent']
    data_dict = {feat: val for feat, val in zip(features, data_point_original)}

    # Simple logic to determine disaster type
    disaster_type = "High Risk Event"
    if data_dict['rainfall_mm'] > 25: disaster_type = "Flood"
    elif data_dict['temperature_c'] > 33: disaster_type = "Wildfire"
    elif data_dict['seismic_intensity'] > 4: disaster_type = "Earthquake"

    alerts = {
        "english": f"ALERT: High risk of {disaster_type} detected. Please stay safe.",
        "hindi": f"चेतावनी: {disaster_type} का उच्च जोखिम पाया गया है। कृपया सुरक्षित रहें।",
        "punjabi": f"ਚੇਤਾਵਨੀ: {disaster_type} ਦਾ ਵੱਧ ਖਤਰਾ ਦੇਖਿਆ ਗਿਆ ਹੈ। ਕਿਰਪਾ ਕਰਕੇ ਸੁਰੱਖਿਅਤ ਰਹੋ।"
    }

    print(f"Detected Disaster Type: {disaster_type}")
    for lang, msg in alerts.items():
        print(f"[{lang.capitalize()}]: {msg}")

# Run the simulation
simulate_multilingual_alert(final_prediction, sample_data_point_scaled, scaler)

# ### Step 4: Rescue Coordination and Routing Simulation
def simulate_rescue_routing():
    """Simulates finding the optimal route for a rescue team using a graph."""
    print("\n--- Simulating Rescue Team Coordination ---")

    G = nx.grid_2d_graph(6, 6) # Create a 6x6 grid map
    for (u, v) in G.edges():
        G.edges[u, v]['weight'] = random.randint(1, 10)

    rescue_base = (0, 0)
    disaster_location = (5, 5)

    # Simulate blocked paths (e.g., collapsed bridge, wildlife corridor)
    blocked_paths = [((2, 2), (2, 3)), ((3, 4), (4, 4))]
    for path in blocked_paths:
        if G.has_edge(*path):
            G.remove_edge(*path)

    try:
        shortest_path = nx.dijkstra_path(G, source=rescue_base, target=disaster_location, weight='weight')
        path_length = nx.dijkstra_path_length(G, source=rescue_base, target=disaster_location, weight='weight')
        print(f"Optimal Safe Route Found: {' -> '.join(map(str, shortest_path))}")
        print(f"Total Route Weight (e.g., time/distance): {path_length}")

        # Visualization
        pos = {(x, y): (x, y) for x, y in G.nodes()}
        nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500)
        path_edges = list(zip(shortest_path, shortest_path[1:]))
        nx.draw_networkx_nodes(G, pos, nodelist=[rescue_base, disaster_location], node_color='red')
        nx.draw_networkx_edges(G, pos, edgelist=path_edges, edge_color='red', width=2)
        plt.title("Optimal Rescue Route")
        plt.show()
    except nx.NetworkXNoPath:
        print("No safe path found to the disaster location.")

# Run the routing simulation
simulate_rescue_routing()

# ### Step 5: API Prototyping (using Flask)
# Below is a simple example of how you could wrap your model into a Flask API. This code would typically live in its own `api.py` script but is prototyped here for demonstration.
#
# **To run this, you would need to install Flask (`pip install Flask`) and run the cell. Then you could send a POST request to `http://127.0.0.1:5000/predict` with JSON data.**

'''
from flask import Flask, request, jsonify
import numpy as np
import utils  # Assumes utils.py is in the same directory

app = Flask(__name__)

# Load models and scaler once at startup
models = {
    'rf': utils.load_model('random_forest_v1.pkl'),
    'lstm': utils.load_model('lstm_model_v1.h5'),
    'transformer': utils.load_model('transformer_model_v1.h5')
}
with open('../data/processed/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    # Expected input: {'features': [val1, val2, ...]}
    features = np.array(data['features']).reshape(1, -1)

    # Scale the features
    scaled_features = scaler.transform(features)

    # Get ensemble prediction
    y_prob_rf = models['rf'].predict_proba(scaled_features)[:, 1]
    scaled_features_dl = scaled_features.reshape(1, 1, -1)
    y_prob_lstm = models['lstm'].predict(scaled_features_dl, verbose=0).flatten()
    y_prob_trans = models['transformer'].predict(scaled_features_dl, verbose=0).flatten()

    ensemble_prob = (y_prob_rf[0] * 0.4) + (y_prob_lstm[0] * 0.3) + (y_prob_trans[0] * 0.3)
    prediction = 1 if ensemble_prob > 0.5 else 0

    return jsonify({
        'prediction': prediction,
        'probability': float(ensemble_prob)
    })

if __name__ == '__main__':
    app.run(port=5000, debug=True)
'''