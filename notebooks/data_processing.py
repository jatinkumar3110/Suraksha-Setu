# -*- coding: utf-8 -*-
"""Data Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VESPbZM9ELJR-wiuEGaAxd6I0_n5MK5G
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest

def generate_disaster_data(samples=2500, test_anomalies=50):
    """
    Generates a realistic, time-indexed mock dataset for disaster prediction.

    Args:
        samples (int): Total number of data points to generate.
        test_anomalies (int): Number of anomalous data points to inject.

    Returns:
        pd.DataFrame: A dataframe containing the simulated sensor data.
    """
    print("Generating simulated dataset...")
    timestamps = pd.to_datetime(pd.date_range(start='2023-01-01', periods=samples, freq='H'))

    data = {
        'rainfall_mm': np.random.uniform(0, 20, size=samples) + np.sin(np.arange(samples)/100) * 10,
        'river_level_m': np.random.uniform(1, 5, size=samples) + np.sin(np.arange(samples)/150) * 2,
        'seismic_intensity': np.random.uniform(0, 2, size=samples),
        'citizen_reports': np.random.randint(0, 5, size=samples),
        'temperature_c': 25 + 10 * np.sin(np.arange(samples)/(24*7)),
        'humidity_percent': 60 - 20 * np.sin(np.arange(samples)/(24*7)),
    }
    df = pd.DataFrame(data, index=timestamps)

    # Inject disaster conditions
    df.loc[df[(df['rainfall_mm'] > 25) & (df['river_level_m'] > 6)].index, 'disaster_risk'] = 1
    df.loc[df[(df['temperature_c'] > 33) & (df['humidity_percent'] < 40)].index, 'disaster_risk'] = 1
    quake_risk_indices = df[df['seismic_intensity'] > 1.8].index
    df.loc[quake_risk_indices, 'seismic_intensity'] = np.random.uniform(4, 9, size=len(quake_risk_indices))
    df.loc[quake_risk_indices, 'disaster_risk'] = 1
    df['disaster_risk'].fillna(0, inplace=True)

    # Inject anomalies
    anomaly_indices = df.sample(n=test_anomalies, random_state=42).index
    df.loc[anomaly_indices, 'rainfall_mm'] = np.random.uniform(300, 500, size=test_anomalies)
    df['is_anomaly'] = 0
    df.loc[anomaly_indices, 'is_anomaly'] = 1

    print("Dataset generated successfully.")
    return df.sample(frac=1, random_state=42)

def detect_anomalies(df):
    """
    Uses an Isolation Forest model to detect and flag anomalies in the dataset.

    Args:
        df (pd.DataFrame): The input dataframe.

    Returns:
        pd.DataFrame: The dataframe with an 'anomaly_prediction' column.
    """
    print("Detecting anomalies...")
    features = ['rainfall_mm', 'river_level_m', 'seismic_intensity', 'citizen_reports', 'temperature_c', 'humidity_percent']
    iso_forest = IsolationForest(contamination='auto', random_state=42)
    df['anomaly_prediction'] = iso_forest.fit_predict(df[features])
    df['anomaly_prediction'] = df['anomaly_prediction'].apply(lambda x: 1 if x == -1 else 0)
    print("Anomaly detection complete.")
    return df

def preprocess_for_modeling(df):
    """
    Splits and scales the data for machine learning modeling.

    Args:
        df (pd.DataFrame): The dataframe to process.

    Returns:
        Tuple: Contains scaled training and testing data splits (X_train, X_test, y_train, y_test) and the scaler object.
    """
    print("Preprocessing data for modeling...")
    features = ['rainfall_mm', 'river_level_m', 'seismic_intensity', 'citizen_reports', 'temperature_c', 'humidity_percent']
    X = df[features]
    y = df['disaster_risk']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print("Preprocessing complete.")
    return X_train_scaled, X_test_scaled, y_train.values, y_test.values, scaler

