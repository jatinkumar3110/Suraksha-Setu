# -*- coding: utf-8 -*-
"""Notebook 1 - Data Exploration Script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gRLRodNNxbEpLZWIcwY1Hat3XmT8Nhrp
"""

# # Notebook 1: Data Exploration and Preprocessing
#
# **Objective:** To load the raw data, perform exploratory data analysis (EDA), visualize key features, detect anomalies, and preprocess the data for modeling.

#Step 1: Import Libraries and Load Data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

# Add the src directory to the system path to import our custom modules
sys.path.append(os.path.abspath(os.path.join('..', 'src')))

import data_processing as dp

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')

# Generate our raw data using the function from our script
raw_df = dp.generate_disaster_data()

# Save to a CSV to simulate a real-world scenario
if not os.path.exists('../data/raw'):
    os.makedirs('../data/raw')
raw_df.to_csv('../data/raw/simulated_sensor_data.csv')

print("Data loaded and saved. Shape:", raw_df.shape)
print(raw_df.head())

# ### Step 2: Exploratory Data Analysis (EDA)
print(raw_df.info())
print("\nStatistical Summary:")
print(raw_df.describe())

# #### Visualizing Feature Distributions
raw_df.drop(['disaster_risk', 'is_anomaly'], axis=1).hist(bins=30, figsize=(15, 10), layout=(2, 3))
plt.suptitle("Feature Distributions")
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# #### Visualizing Correlation Matrix
plt.figure(figsize=(10, 8))
correlation_matrix = raw_df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=".2f")
plt.title("Feature Correlation Matrix")
plt.show()

# ### Step 3: Anomaly Detection
# We'll use the function from our script to identify and flag potential anomalies.
df_anomalies = dp.detect_anomalies(raw_df.copy())

print("\nAnomaly Detection Results (Actual vs. Predicted):")
print(pd.crosstab(df_anomalies['is_anomaly'], df_anomalies['anomaly_prediction']))

print("\nSample of records flagged as anomalies:")
print(df_anomalies[df_anomalies['anomaly_prediction'] == 1].head())

# ### Step 4: Preprocess Data for Modeling
# Now we split the data into training and testing sets and scale the features.
X_train, X_test, y_train, y_test, scaler = dp.preprocess_for_modeling(df_anomalies)

print(f"\nX_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# ### Step 5: Save Processed Data
# Save the final processed data splits to be used in the next notebook.
processed_dir = '../data/processed'
if not os.path.exists(processed_dir):
    os.makedirs(processed_dir)

np.save(os.path.join(processed_dir, 'X_train.npy'), X_train)
np.save(os.path.join(processed_dir, 'X_test.npy'), X_test)
np.save(os.path.join(processed_dir, 'y_train.npy'), y_train)
np.save(os.path.join(processed_dir, 'y_test.npy'), y_test)

import pickle
with open(os.path.join(processed_dir, 'scaler.pkl'), 'wb') as f:
    pickle.dump(scaler, f)

print("Processed data saved to '../data/processed/' directory.")

