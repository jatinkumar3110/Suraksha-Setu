{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b94d20",
   "metadata": {},
   "source": [
    "# Suraksha Setu â€” Advanced Multi-Hazard Models (Optuna + SMOTE + Feature Importance)\n",
    "\n",
    "**This notebook includes**:\n",
    "- Data loading for Flood, Landslide, and Forest Fire cleaned CSVs\n",
    "- Preprocessing and feature engineering\n",
    "- Class imbalance handling with SMOTE\n",
    "- Model training with hyperparameter tuning using Optuna\n",
    "  - Flood: LSTM (Keras) with Optuna tuner-like loop\n",
    "  - Landslide: XGBoost with Optuna tuning\n",
    "  - Forest Fire: CNN-LSTM with Optuna tuning\n",
    "- Model saving (`.h5`, `.joblib`, `.json`)\n",
    "- Stacking ensemble (XGBoost meta-model)\n",
    "- Evaluation: ROC, Precision-Recall, confusion matrices, calibration, and feature importance\n",
    "- Notes: Align time/spatial indices before real-world stacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e58689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "# !pip install xgboost tensorflow scikit-learn pandas numpy matplotlib seaborn joblib imbalanced-learn optuna tensorflow-addons\n",
    "\n",
    "import os, random\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - update if needed\n",
    "FLOOD_PATH = '/mnt/data/flood_cleaned.csv'\n",
    "LANDSLIDE_PATH = '/mnt/data/Landslide_Factors_cleaned.csv'\n",
    "FIRE_PATH = '/mnt/data/forestfires_cleaned.csv'\n",
    "OUTPUT_DIR = '/mnt/data/suraksha_models_advanced'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('OUTPUT_DIR =', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e68052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from sklearn.calibration import calibration_curve\n",
    "def print_metrics(y_true, y_pred_prob, threshold=0.5):\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    ap = average_precision_score(y_true, y_pred_prob)\n",
    "    print(f'Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | AUC: {auc:.3f} | AP: {ap:.3f}')\n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1':f1,'auc':auc,'ap':ap}\n",
    "\n",
    "def plot_roc_pr(y_true, y_scores, label='Model'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    ap = average_precision_score(y_true, y_scores)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(fpr, tpr, label=f'{label} (AUC={auc:.3f})')\n",
    "    plt.plot([0,1],[0,1],'k--', alpha=0.5)\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(recall, precision, label=f'{label} (AP={ap:.3f})')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall'); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def save_keras_model(model, path):\n",
    "    model.save(path)\n",
    "    print('Saved Keras model to', path)\n",
    "\n",
    "def save_joblib(obj, path):\n",
    "    joblib.dump(obj, path)\n",
    "    print('Saved joblib object to', path)\n",
    "\n",
    "def plot_calibration(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Calibration')\n",
    "    plt.plot([0,1],[0,1],'k--', alpha=0.5)\n",
    "    plt.xlabel('Predicted probability'); plt.ylabel('True probability'); plt.title('Calibration curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_flood = pd.read_csv(FLOOD_PATH)\n",
    "df_land = pd.read_csv(LANDSLIDE_PATH)\n",
    "df_fire = pd.read_csv(FIRE_PATH)\n",
    "print('Flood:', df_flood.shape, 'Landslide:', df_land.shape, 'Fire:', df_fire.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flood preprocessing: prepare sequences and apply SMOTE at sequence-level via aggregated features\n",
    "df = df_flood.copy()\n",
    "# detect time and target\n",
    "time_col = next((c for c in df.columns if 'date' in c.lower() or 'time' in c.lower()), None)\n",
    "target_col = next((c for c in df.columns if c.lower() in ['risk','risk_score','target','label','flood_risk']), None)\n",
    "if time_col: df[time_col] = pd.to_datetime(df[time_col], errors='coerce'); df = df.sort_values(time_col)\n",
    "num_feats = df.select_dtypes(include='number').columns.tolist()\n",
    "if target_col in num_feats: num_feats.remove(target_col)\n",
    "print('Using numeric features:', num_feats)\n",
    "# fill missing and scale\n",
    "df[num_feats] = df[num_feats].fillna(df[num_feats].median())\n",
    "scaler_flood = MinMaxScaler()\n",
    "X_num = scaler_flood.fit_transform(df[num_feats])\n",
    "save_joblib(scaler_flood, os.path.join(OUTPUT_DIR,'flood_scaler.joblib'))\n",
    "\n",
    "# create rolling-window aggregates for SMOTE (since SMOTE on sequences is complex)\n",
    "WINDOW = 24\n",
    "agg_features = []\n",
    "for i in range(WINDOW, len(df)):\n",
    "    window = X_num[i-WINDOW:i]\n",
    "    agg = np.concatenate([window.mean(axis=0), window.std(axis=0), window.min(axis=0), window.max(axis=0)])\n",
    "    agg_features.append(agg)\n",
    "agg_features = np.array(agg_features)\n",
    "y = df[target_col].fillna(0).values[WINDOW:] if target_col else (agg_features[:,0] > np.percentile(agg_features[:,0],90)).astype(int)\n",
    "\n",
    "print('Aggregated features shape for SMOTE:', agg_features.shape, 'y shape:', y.shape)\n",
    "sm = SMOTE(random_state=RANDOM_SEED)\n",
    "X_res, y_res = sm.fit_resample(agg_features, y)\n",
    "print('After SMOTE:', X_res.shape, y_res.sum(), 'positive samples')\n",
    "\n",
    "# Convert back to sequences by sampling from resampled aggregates - we'll build sequences by matching nearest aggregate index (approximation)\n",
    "# For simplicity for training LSTM, we'll use the original sequences (without SMOTE) but use class weights computed from y_res\n",
    "from collections import Counter\n",
    "class_weights = {0: (len(y_res)/(2*Counter(y_res)[0])), 1: (len(y_res)/(2*Counter(y_res)[1]))}\n",
    "print('Class weights for flood LSTM:', class_weights)\n",
    "\n",
    "# Build actual sequences (original)\n",
    "SEQ_LEN = 24\n",
    "def create_sequences(X, y, seq_len=SEQ_LEN):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-seq_len):\n",
    "        Xs.append(X[i:(i+seq_len)])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_num, df[target_col].fillna(0).values if target_col else (X_num[:,0] > np.percentile(X_num[:,0],90)).astype(int), SEQ_LEN)\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_flood, X_test_flood = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train_flood, y_test_flood = y_seq[:split_idx], y_seq[split_idx:]\n",
    "print('Flood sequences:', X_train_flood.shape, y_train_flood.sum(), 'positives in train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective for LSTM (search over units, dropout, lr)\n",
    "import optuna, math, tempfile\n",
    "def build_lstm(trial, input_shape):\n",
    "    units1 = trial.suggest_categorical('units1', [32,64,128])\n",
    "    units2 = trial.suggest_categorical('units2', [16,32,64])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.4)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Bidirectional(layers.LSTM(units1, return_sequences=True))(inp)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(units2))(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "def objective_lstm(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_lstm(trial, (X_train_flood.shape[1], X_train_flood.shape[2]))\n",
    "    batch = trial.suggest_categorical('batch_size', [32,64,128])\n",
    "    epochs = 30\n",
    "    history = model.fit(X_train_flood, y_train_flood, validation_split=0.1, epochs=epochs, batch_size=batch, class_weight=class_weights, verbose=0, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "    preds = model.predict(X_test_flood).ravel()\n",
    "    auc = roc_auc_score(y_test_flood, preds)\n",
    "    # save intermediate best model\n",
    "    return auc\n",
    "\n",
    "study_lstm = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "study_lstm.optimize(objective_lstm, n_trials=12)  # adjust n_trials as compute allows\n",
    "print('Best LSTM trial:', study_lstm.best_params, 'AUC:', study_lstm.best_value)\n",
    "\n",
    "# Build final model with best params and save\n",
    "best_params = study_lstm.best_params\n",
    "final_model = build_lstm(optuna.trial.FixedTrial(best_params), (X_train_flood.shape[1], X_train_flood.shape[2]))\n",
    "final_model.fit(X_train_flood, y_train_flood, validation_split=0.1, epochs=50, batch_size=best_params.get('batch_size',64), class_weight=class_weights, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "save_keras_model(final_model, os.path.join(OUTPUT_DIR, 'flood_lstm_opt.h5'))\n",
    "y_pred_prob_lstm = final_model.predict(X_test_flood).ravel()\n",
    "metrics_lstm = print_metrics(y_test_flood, y_pred_prob_lstm)\n",
    "plot_roc_pr(y_test_flood, y_pred_prob_lstm, label='Flood_LSTM_Opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landslide preprocessing with SMOTE and feature scaling\n",
    "df = df_land.copy()\n",
    "target_col = next((c for c in df.columns if c.lower() in ['target','label','landslide','landslide_risk','susceptible']), None)\n",
    "if target_col is None:\n",
    "    # create proxy target\n",
    "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    target_col = 'target'\n",
    "    df[target_col] = (df[num_cols[0]] > np.percentile(df[num_cols[0]],90)).astype(int)\n",
    "print('Target column for landslide:', target_col)\n",
    "# Drop columns with too many missing\n",
    "df = df.loc[:, df.isnull().mean() < 0.4]\n",
    "num_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "if target_col in num_cols: num_cols.remove(target_col)\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "for c in num_cols:\n",
    "    df[c].fillna(df[c].median(), inplace=True)\n",
    "for c in cat_cols:\n",
    "    df[c].fillna('NA', inplace=True); df[c] = LabelEncoder().fit_transform(df[c].astype(str))\n",
    "X = df[num_cols]; y = df[target_col].astype(int)\n",
    "scaler_land = StandardScaler()\n",
    "X_scaled = scaler_land.fit_transform(X)\n",
    "save_joblib(scaler_land, os.path.join(OUTPUT_DIR,'land_scaler.joblib'))\n",
    "sm = SMOTE(random_state=RANDOM_SEED)\n",
    "X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "print('After SMOTE:', X_res.shape, 'Positives:', y_res.sum())\n",
    "X_train_land, X_test_land, y_train_land, y_test_land = train_test_split(X_res, y_res, test_size=0.2, random_state=RANDOM_SEED, stratify=y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286449f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective for XGBoost\n",
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param, use_label_encoder=False, eval_metric='auc', random_state=RANDOM_SEED)\n",
    "    model.fit(X_train_land, y_train_land, eval_set=[(X_test_land, y_test_land)], early_stopping_rounds=20, verbose=False)\n",
    "    preds = model.predict_proba(X_test_land)[:,1]\n",
    "    return roc_auc_score(y_test_land, preds)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "study_xgb.optimize(objective_xgb, n_trials=30)\n",
    "print('Best XGB params:', study_xgb.best_params, 'AUC:', study_xgb.best_value)\n",
    "\n",
    "# Train final XGB with best params\n",
    "best_xgb = xgb.XGBClassifier(**study_xgb.best_params, use_label_encoder=False, eval_metric='auc', random_state=RANDOM_SEED)\n",
    "best_xgb.fit(X_train_land, y_train_land)\n",
    "save_joblib(best_xgb, os.path.join(OUTPUT_DIR,'landslide_xgb.joblib'))\n",
    "\n",
    "# Feature importance\n",
    "fi = pd.DataFrame({'feature': df[num_cols].columns, 'importance': best_xgb.feature_importances_}).sort_values('importance', ascending=False)\n",
    "plt.figure(figsize=(8,6)); sns.barplot(data=fi.head(20), x='importance', y='feature'); plt.title('Feature importance - Landslide XGB'); plt.show()\n",
    "\n",
    "y_pred_prob_xgb = best_xgb.predict_proba(X_test_land)[:,1]\n",
    "metrics_xgb = print_metrics(y_test_land, y_pred_prob_xgb)\n",
    "plot_roc_pr(y_test_land, y_pred_prob_xgb, label='Landslide_XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire data preprocessing: sequences + SMOTE via aggregated features\n",
    "df = df_fire.copy()\n",
    "time_col = next((c for c in df.columns if 'date' in c.lower() or 'time' in c.lower()), None)\n",
    "target_col = next((c for c in df.columns if c.lower() in ['fire_risk','risk','hotspot','target','label']), None)\n",
    "if time_col: df[time_col] = pd.to_datetime(df[time_col], errors='coerce'); df = df.sort_values(time_col)\n",
    "num_feats = df.select_dtypes('number').columns.tolist()\n",
    "if target_col in num_feats: num_feats.remove(target_col)\n",
    "df[num_feats] = df[num_feats].fillna(df[num_feats].median())\n",
    "scaler_fire = MinMaxScaler(); X_num = scaler_fire.fit_transform(df[num_feats]); save_joblib(scaler_fire, os.path.join(OUTPUT_DIR,'fire_scaler.joblib'))\n",
    "\n",
    "# aggregated features for SMOTE\n",
    "WINDOW = 12\n",
    "agg = []\n",
    "for i in range(WINDOW, len(df)):\n",
    "    w = X_num[i-WINDOW:i]\n",
    "    agg.append(np.concatenate([w.mean(axis=0), w.std(axis=0)]))\n",
    "agg = np.array(agg)\n",
    "y_agg = df[target_col].fillna(0).values[WINDOW:] if target_col else (agg[:,0] > np.percentile(agg[:,0],90)).astype(int)\n",
    "sm = SMOTE(random_state=RANDOM_SEED); X_res_f, y_res_f = sm.fit_resample(agg, y_agg)\n",
    "print('After SMOTE (fire):', X_res_f.shape, 'positives:', y_res_f.sum())\n",
    "\n",
    "# Build sequences for CNN-LSTM (use original sequences for training, class weights from SMOTE)\n",
    "SEQ_LEN = 12\n",
    "def create_seq(X,y,seq_len=SEQ_LEN):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-seq_len):\n",
    "        Xs.append(X[i:(i+seq_len)])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "X_seq_f, y_seq_f = create_seq(X_num, df[target_col].fillna(0).values if target_col else (X_num[:,0] > np.percentile(X_num[:,0],90)).astype(int), SEQ_LEN)\n",
    "split_idx = int(0.8 * len(X_seq_f))\n",
    "X_train_fire, X_test_fire = X_seq_f[:split_idx], X_seq_f[split_idx:]\n",
    "y_train_fire, y_test_fire = y_seq_f[:split_idx], y_seq_f[split_idx:]\n",
    "from collections import Counter\n",
    "cw_fire = {0: (len(y_res_f)/(2*Counter(y_res_f)[0])), 1: (len(y_res_f)/(2*Counter(y_res_f)[1]))}\n",
    "print('Fire sequences:', X_train_fire.shape, 'class weights:', cw_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a890526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna for CNN-LSTM fire model\n",
    "def build_fire_model(trial, input_shape):\n",
    "    conv_filters = trial.suggest_categorical('conv_filters', [16,32,64])\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [32,64])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.4)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.TimeDistributed(layers.Conv1D(filters=conv_filters, kernel_size=3, activation='relu'))(inp)\n",
    "    x = layers.TimeDistributed(layers.MaxPool1D(2))(x)\n",
    "    x = layers.TimeDistributed(layers.Flatten())(x)\n",
    "    x = layers.LSTM(lstm_units)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "def objective_fire(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_fire_model(trial, (X_train_fire.shape[1], X_train_fire.shape[2]))\n",
    "    batch = trial.suggest_categorical('batch_size', [32,64])\n",
    "    history = model.fit(X_train_fire, y_train_fire, validation_split=0.1, epochs=30, batch_size=batch, class_weight=cw_fire, verbose=0, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "    preds = model.predict(X_test_fire).ravel()\n",
    "    return roc_auc_score(y_test_fire, preds)\n",
    "\n",
    "study_fire = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "study_fire.optimize(objective_fire, n_trials=12)\n",
    "print('Best fire params:', study_fire.best_params, 'AUC:', study_fire.best_value)\n",
    "\n",
    "# Train final fire model with best params\n",
    "best_fire_params = study_fire.best_params\n",
    "final_fire = build_fire_model(optuna.trial.FixedTrial(best_fire_params), (X_train_fire.shape[1], X_train_fire.shape[2]))\n",
    "final_fire.fit(X_train_fire, y_train_fire, validation_split=0.1, epochs=50, batch_size=best_fire_params.get('batch_size',32), class_weight=cw_fire, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "save_keras_model(final_fire, os.path.join(OUTPUT_DIR,'fire_cnn_lstm_opt.h5'))\n",
    "y_pred_prob_fire = final_fire.predict(X_test_fire).ravel()\n",
    "metrics_fire = print_metrics(y_test_fire, y_pred_prob_fire)\n",
    "plot_roc_pr(y_test_fire, y_pred_prob_fire, label='Fire_CNN_LSTM_Opt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking ensemble - ALIGN DATA BEFORE USING IN PRODUCTION!\n",
    "# Here we create a demo meta-dataset by taking minimal overlapping samples.\n",
    "len_f = len(y_test_flood) if 'y_test_flood' in globals() else 0\n",
    "len_l = len(y_test_land) if 'y_test_land' in globals() else 0\n",
    "len_fire = len(y_test_fire) if 'y_test_fire' in globals() else 0\n",
    "min_len = min(len_f, len_l, len_fire) if min([len_f,len_l,len_fire])>0 else None\n",
    "print('Test lengths (flood, land, fire):', len_f, len_l, len_fire, 'min_len:', min_len)\n",
    "\n",
    "if min_len:\n",
    "    meta_X = pd.DataFrame({\n",
    "        'flood_prob': y_pred_prob_lstm[:min_len],\n",
    "        'landslide_prob': y_pred_prob_xgb[:min_len],\n",
    "        'fire_prob': y_pred_prob_fire[:min_len]\n",
    "    })\n",
    "    meta_y = y_test_flood[:min_len]  # proxy; align properly in real use\n",
    "    meta_model = xgb.XGBClassifier(n_estimators=200, max_depth=3, random_state=RANDOM_SEED, use_label_encoder=False, eval_metric='logloss')\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "    save_joblib(meta_model, os.path.join(OUTPUT_DIR,'meta_xgb.joblib'))\n",
    "    meta_prob = meta_model.predict_proba(meta_X)[:,1]\n",
    "    metrics_meta = print_metrics(meta_y, meta_prob)\n",
    "    plot_roc_pr(meta_y, meta_prob, label='Meta_Ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dff607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of model performance and visualizations\n",
    "metrics_dict = {}\n",
    "if 'metrics_lstm' in globals(): metrics_dict['Flood_LSTM'] = metrics_lstm\n",
    "if 'metrics_xgb' in globals(): metrics_dict['Landslide_XGB'] = metrics_xgb\n",
    "if 'metrics_fire' in globals(): metrics_dict['Fire_CNN_LSTM'] = metrics_fire\n",
    "if 'metrics_meta' in globals(): metrics_dict['Meta_Ensemble'] = metrics_meta\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_dict).T\n",
    "display(df_metrics)\n",
    "\n",
    "# Plot AUC and AP\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); df_metrics['auc'].plot(kind='bar', color='teal'); plt.title('AUC by Model'); plt.ylim(0,1)\n",
    "plt.subplot(1,2,2); df_metrics['ap'].plot(kind='bar', color='orange'); plt.title('Average Precision (AP)'); plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "# Calibration plots\n",
    "for name, m in metrics_dict.items():\n",
    "    probs = globals().get('y_pred_prob_'+ name.split('_')[0].lower(), None)\n",
    "    true = globals().get('y_test_'+ name.split('_')[0].lower(), None)\n",
    "    if probs is not None and true is not None:\n",
    "        print('Calibration for', name)\n",
    "        plot_calibration(true, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058969f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scalers and list files\n",
    "print('Saved models in', OUTPUT_DIR)\n",
    "print('\\nFiles:'); print('\\n'.join(os.listdir(OUTPUT_DIR)))\n",
    "\n",
    "print('\\nNotes:')\n",
    "print('- This notebook performs Optuna hyperparameter search; adjust n_trials for better tuning (compute/time tradeoff).')\n",
    "print('- Before building a production meta-model, align samples by spatial/time keys (e.g., grid cell + timestamp).')\n",
    "print('- Consider model calibration, ensembling weights, and uncertainty estimation for high-stakes alerts.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
